"""
GoRide Sentiment Analysis - Utility Functions
============================================

This module contains all utility functions for the GoRide Sentiment Analysis application,
including data preprocessing, model training/loading, and analysis tools.

Author: SentimenGo App
Version: 2.0.0 (Rebuilt)
"""

# ==============================================================================
# IMPORTS
# ==============================================================================

import warnings
import re
import os
import sys
import time
import base64
import traceback
import pickle
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import Counter

# Suppress warnings
warnings.filterwarnings('ignore', message='pkg_resources is deprecated as an API')

# Third-party imports
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import joblib
import nltk
from wordcloud import WordCloud

# Scikit-learn imports
import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.pipeline import Pipeline

# Imbalanced-learn imports
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# NLTK imports
from nltk.util import ngrams
from nltk.tokenize import word_tokenize

# Indonesian language processing
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

# ==============================================================================
# NLTK SETUP
# ==============================================================================

# ==============================================================================
# NLTK SETUP - Enhanced for Streamlit Cloud
# ==============================================================================

def ensure_nltk_data():
    """Ensure NLTK data is available for Streamlit Cloud deployment"""
    import nltk
    import streamlit as st
    
    # List of required NLTK data
    required_nltk_data = [
        ('tokenizers/punkt', 'punkt'),
        ('tokenizers/punkt_tab', 'punkt_tab'),
        ('corpora/stopwords', 'stopwords'),
    ]
    
    missing_data = []
    
    for data_path, data_name in required_nltk_data:
        try:
            nltk.data.find(data_path)
        except LookupError:
            missing_data.append(data_name)
    
    if missing_data:
        with st.spinner(f'üì• Downloading NLTK data: {", ".join(missing_data)}...'):
            for data_name in missing_data:
                try:
                    nltk.download(data_name, quiet=True)
                    st.success(f'‚úÖ Downloaded NLTK {data_name}')
                except Exception as e:
                    st.warning(f'‚ö†Ô∏è Failed to download NLTK {data_name}: {str(e)}')
                    # Try alternative approach
                    try:
                        nltk.download(data_name, quiet=False)
                    except Exception as e2:
                        st.error(f'‚ùå Critical error downloading NLTK {data_name}: {str(e2)}')

# Initialize NLTK data
try:
    ensure_nltk_data()
except Exception as e:
    import streamlit as st
    st.warning(f'‚ö†Ô∏è NLTK initialization warning: {str(e)}')

# ==============================================================================
# CONSTANTS AND CONFIGURATION
# ==============================================================================

# Directory configuration
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
DICTIONARY_DIR = BASE_DIR / "data"
MODEL_DIR = BASE_DIR / "models"

# Default preprocessing options
DEFAULT_PREPROCESSING_OPTIONS = {
    'case_folding': True,
    'phrase_standardization': True,
    'cleansing': True,
    'normalize_slang': True,
    'remove_repeated': True,
    'tokenize': True,
    'remove_stopwords': True,
    'stemming': True,
    'rejoin': True
}

# Label mapping for sentiment consistency
LABEL_MAP = {
    'Positive': 'POSITIF', 'POSITIVE': 'POSITIF',
    'Negative': 'NEGATIF', 'NEGATIVE': 'NEGATIF',
    'Netral': 'NETRAL', 'Neutral': 'NETRAL', 'NETRAL': 'NETRAL', 'NEUTRAL': 'NETRAL'
}

# Required columns for data validation
REQUIRED_COLUMNS = ['review_text', 'sentiment', 'date', 'teks_preprocessing']

# ==============================================================================
# INDONESIAN LANGUAGE PROCESSING SETUP
# ==============================================================================

# Initialize Indonesian language processors
factory = StemmerFactory()
stemmer = factory.create_stemmer()

stop_factory = StopWordRemoverFactory()
stopword = stop_factory.create_stop_word_remover()
stopword_list = set(stop_factory.get_stop_words())

# Load dictionaries
slang_path = DICTIONARY_DIR / "kamus_slang_formal.txt"
stopwords_path = DICTIONARY_DIR / "stopwordsID.txt"

# Load slang dictionary
if not slang_path.exists():
    slang_path = BASE_DIR / "kamus_slang_formal.txt"

try:
    with open(slang_path, 'r', encoding='utf-8') as f:
        slang_dict = dict(line.strip().split(':') for line in f if ':' in line.strip())
except FileNotFoundError:
    slang_dict = {}

# Load custom stopwords
if not stopwords_path.exists():
    stopwords_path = BASE_DIR / "stopwordsID.txt"

try:
    with open(stopwords_path, 'r', encoding='utf-8') as f:
        custom_stopwords = set(f.read().splitlines())
    stopword_list.update(custom_stopwords)
except FileNotFoundError:
    pass

# ==============================================================================
# TEXT PREPROCESSING FUNCTIONS
# ==============================================================================

def normalize_word(word: str) -> str:
    """
    Normalize a single word using the slang dictionary.
    
    Args:
        word: Word to normalize
        
    Returns:
        Normalized word or original word if not found in dictionary
    """
    if not isinstance(word, str) or not word.strip():
        return word
    
    if slang_dict:
        normalized = slang_dict.get(word.lower(), word)
        return normalized if normalized is not None else word
    return word


def preprocess_text(text: Union[str, None], options: Optional[Dict] = None) -> str:
    """
    Comprehensive text preprocessing following the exact steps from notebook.
    
    Args:
        text: Text to preprocess
        options: Preprocessing options dictionary
        
    Returns:
        Preprocessed text string
    """
    # Input validation
    if text is None:
        return ""
    
    if not isinstance(text, str):
        try:
            text = str(text)
        except Exception:
            return ""
    
    if not text.strip():
        return ""
    
    # Set default options
    if options is None:
        options = DEFAULT_PREPROCESSING_OPTIONS.copy()
    
    try:
        # Step 1: Case Folding + Phrase Standardization
        if options.get('case_folding', True):
            text = text.lower()
        
        if options.get('phrase_standardization', True):
            # Normalize common terms: "go ride", "go-ride", etc. ‚Üí "goride"
            text = re.sub(r'\bgo[\s\-_]?ride\b', 'goride', text)
            # Additional normalizations can be added here if needed
        
        # Step 2: Cleansing
        if options.get('cleansing', True):
            # Remove URLs
            text = re.sub(r'http\S+|www\S+', '', text)
            # Remove non-alphabetic characters except spaces
            text = re.sub(r'[^a-zA-Z\s]', ' ', text)
            # Normalize whitespace
            text = re.sub(r'\s+', ' ', text).strip()
        
        # Step 3: Slang Normalization
        if options.get('normalize_slang', True) and slang_dict:
            words = text.split()
            normalized_words = []
            for word in words:
                if word.strip():
                    normalized_word = slang_dict.get(word, word)
                    if normalized_word and normalized_word.strip():
                        normalized_words.append(normalized_word)
            text = ' '.join(normalized_words)
        
        # Step 4: Remove Repeated Characters
        if options.get('remove_repeated', True):
            # Keep maximum 2 repeated characters
            text = re.sub(r'(\w)\1{2,}', r'\1\1', text)
        
        # Step 5: Tokenization
        if options.get('tokenize', True):
            tokens = re.findall(r'\b\w+\b', text)
        else:
            tokens = text.split() if isinstance(text, str) else [text]
        
        # Step 6: Stopword Removal
        if options.get('remove_stopwords', True) and isinstance(tokens, list):
            filtered_tokens = []
            for word in tokens:
                if word and word not in stopword_list and len(word) > 1:
                    filtered_tokens.append(word)
            tokens = filtered_tokens
        
        # Step 7: Stemming
        if options.get('stemming', True) and isinstance(tokens, list):
            stemmed_tokens = []
            for word in tokens:
                if word and word.strip():
                    try:
                        stemmed_word = stemmer.stem(word)
                        if stemmed_word and stemmed_word.strip():
                            stemmed_tokens.append(stemmed_word)
                    except Exception:
                        # If stemming fails, use original word
                        stemmed_tokens.append(word)
            tokens = stemmed_tokens
          # Step 8: Rejoin Tokens
        if options.get('rejoin', True) and isinstance(tokens, list):
            # Filter out None values and empty strings before joining
            filtered_tokens = [token for token in tokens if token is not None and str(token).strip()]
            return ' '.join(filtered_tokens)
        
        return ' '.join(tokens) if isinstance(tokens, list) else str(tokens)
        
    except Exception as e:
        # Return original text or empty string if error occurs
        try:
            return str(text) if text is not None else ""
        except Exception:
            return ""

# ==============================================================================
# TEXT ANALYSIS FUNCTIONS
# ==============================================================================

def get_word_frequencies(text: Union[str, List[str]], top_n: int = 10) -> Dict[str, int]:
    """
    Get word frequencies from text.
    
    Args:
        text: Text string or list of words
        top_n: Number of top frequent words to return
        
    Returns:
        Dictionary of word frequencies
    """
    try:
        words = nltk.word_tokenize(text) if isinstance(text, str) else text
        word_freq = Counter(words)
        return dict(word_freq.most_common(top_n))
    except Exception as e:
        st.error(f"Error in word frequency analysis: {str(e)}")
        return {}


def get_ngrams(text: Union[str, List[str]], n: int, top_n: int = 10) -> Dict[str, int]:
    """
    Get n-grams from text.
    
    Args:
        text: Text string or list of words
        n: N-gram size (2 for bigrams, 3 for trigrams, etc.)
        top_n: Number of top n-grams to return
        
    Returns:
        Dictionary of n-gram frequencies
    """
    try:
        words = nltk.word_tokenize(text) if isinstance(text, str) else text
        n_grams = list(ngrams(words, n))
        n_gram_freq = Counter([' '.join(g) for g in n_grams])
        return dict(n_gram_freq.most_common(top_n))
    except Exception as e:
        st.error(f"Error in n-gram analysis: {str(e)}")
        return {}


def create_wordcloud(text: Union[str, List[str]], max_words: int = 100, 
                    background_color: str = 'white') -> Optional[Any]:
    """
    Create wordcloud with proper error handling and type checking.
    
    Args:
        text: Text to create wordcloud from
        max_words: Maximum number of words in wordcloud
        background_color: Background color for wordcloud
        
    Returns:
        WordCloud object or None if error occurs
    """
    try:
        # Ensure text is a string
        if isinstance(text, list):
            text = ' '.join(filter(None, text))  # Filter out None values
        elif not isinstance(text, str):
            text = str(text)
        
        # Check if text is empty after processing
        if not text.strip():
            return None
            
        wordcloud = WordCloud(
            width=800,
            height=400,
            max_words=max_words,
            background_color=background_color,
            colormap='viridis',
            contour_width=1,
            contour_color='steelblue'
        ).generate(text)
        return wordcloud
    except Exception as e:
        st.error(f"Error generating word cloud: {str(e)}")
        return None


def analyze_sentiment_trends(data: pd.DataFrame) -> pd.DataFrame:
    """
    Analyze sentiment trends over time.
    
    Args:
        data: DataFrame with sentiment and date columns
        
    Returns:
        DataFrame with sentiment trends
    """
    try:
        data['date'] = pd.to_datetime(data['date'], errors='coerce')
        data = data.dropna(subset=['date'])
        
        sentiment_trends = data.groupby([
            data['date'].dt.strftime('%Y-%m-%d'), 'sentiment'
        ]).size().reset_index(name='count')
        
        pivot_trends = sentiment_trends.pivot(
            index='date', columns='sentiment', values='count'
        ).fillna(0)
        
        if 'POSITIF' in pivot_trends.columns and 'NEGATIF' in pivot_trends.columns:
            pivot_trends['ratio'] = pivot_trends['POSITIF'] / (
                pivot_trends['POSITIF'] + pivot_trends['NEGATIF']
            )
        
        return pivot_trends
    except Exception as e:
        st.error(f"Error in trend analysis: {str(e)}")
        return pd.DataFrame()

# ==============================================================================
# DATA LOADING AND PREPROCESSING FUNCTIONS
# ==============================================================================

def prepare_and_load_preprocessed_data(max_rows: Optional[int] = None, 
                                     chunksize: int = 10000, 
                                     preprocessing_options: Optional[Dict] = None) -> pd.DataFrame:
    """
    Load preprocessed data if available, otherwise perform batch preprocessing and save.
    
    Args:
        max_rows: Maximum number of rows to load
        chunksize: Batch size for processing
        preprocessing_options: Preprocessing configuration
        
    Returns:
        DataFrame with preprocessed data
    """
    preprocessed_path = DATA_DIR / "ulasan_goride_preprocessed.csv"
    raw_path = DATA_DIR / "ulasan_goride.csv"
    
    if preprocessing_options is None:
        preprocessing_options = DEFAULT_PREPROCESSING_OPTIONS.copy()
    
    # Load preprocessed data if available
    if preprocessed_path.exists():
        try:
            df = pd.read_csv(preprocessed_path, nrows=max_rows)
            
            # Ensure label mapping is correct
            df['sentiment'] = df['sentiment'].replace(LABEL_MAP)
            df = df[df['sentiment'].isin(['POSITIF', 'NEGATIF'])]
            
            # Validate required columns
            for col in REQUIRED_COLUMNS:
                if col not in df.columns:
                    st.error(f"Column {col} not found in preprocessed file!")
                    return pd.DataFrame(columns=REQUIRED_COLUMNS)
            
            return df
        except Exception as e:
            st.error(f"Failed to read preprocessed file: {str(e)}")
            # Remove corrupted preprocessed file
            try:
                preprocessed_path.unlink()
            except Exception:
                pass
    
    # Perform batch preprocessing if raw data exists
    if not raw_path.exists():
        st.error("File ulasan_goride.csv not found!")
        return pd.DataFrame(columns=REQUIRED_COLUMNS)
    
    try:
        df = pd.read_csv(raw_path, nrows=max_rows)
        
        # Validate required columns in raw data
        required_raw_columns = ['review_text', 'sentiment', 'date']
        for col in required_raw_columns:
            if col not in df.columns:
                st.error(f"Column {col} not found in CSV file!")
                return pd.DataFrame(columns=REQUIRED_COLUMNS)
        
        # Map sentiment labels
        df['sentiment'] = df['sentiment'].replace(LABEL_MAP)
        df = df[df['sentiment'].isin(['POSITIF', 'NEGATIF'])]
        
        # Batch preprocessing
        with st.spinner("Performing batch preprocessing and saving results..."):
            df['teks_preprocessing'] = df['review_text'].astype(str).apply(
                lambda x: preprocess_text(x, preprocessing_options)
            )
            # Save preprocessed data
            df.to_csv(preprocessed_path, index=False)
        
        return df
    except Exception as e:
        st.error(f"Failed to perform batch preprocessing: {str(e)}")
        return pd.DataFrame(columns=REQUIRED_COLUMNS)


@st.cache_data(ttl=1800)
def load_sample_data(max_rows: Optional[int] = None, chunksize: int = 10000) -> pd.DataFrame:
    """
    Cached wrapper for loading sample data.
    
    Args:
        max_rows: Maximum number of rows to load
        chunksize: Batch size for processing
        
    Returns:
        DataFrame with sample data
    """
    return prepare_and_load_preprocessed_data(max_rows=max_rows, chunksize=chunksize)

# ==============================================================================
# MODEL TRAINING FUNCTIONS
# ==============================================================================

@st.cache_resource(ttl=3600)
def train_model(data: pd.DataFrame, preprocessing_options: Optional[Dict] = None, 
               batch_size: int = 1000) -> Tuple:
    """
    Train sentiment analysis model with progress feedback.
    
    Args:
        data: Training data
        preprocessing_options: Preprocessing configuration
        batch_size: Batch size for processing
        
    Returns:
        Tuple of (pipeline, accuracy, precision, recall, f1, confusion_matrix, X_test, y_test, tfidf, svm)
    """
    if preprocessing_options is None:
        preprocessing_options = DEFAULT_PREPROCESSING_OPTIONS.copy()
    
    try:
        progress_bar = st.progress(0)
        status_text = st.empty()
        processed_texts = []
        total_batches = (len(data) + batch_size - 1) // batch_size
        
        # Batch preprocessing
        for i in range(0, len(data), batch_size):
            batch_end = min(i + batch_size, len(data))
            batch = data.iloc[i:batch_end]
            batch_num = i // batch_size + 1
            
            status_text.text(f"Preprocessing batch {batch_num}/{total_batches}...")
            progress_bar.progress(i / len(data))
            
            batch_processed = []
            for text in batch['review_text']:
                try:
                    processed = preprocess_text(text, preprocessing_options)
                    batch_processed.append(processed)
                except Exception:
                    batch_processed.append(str(text))
            
            processed_texts.extend(batch_processed)
        
        status_text.text("Splitting data before TF-IDF (preventing data leakage)...")
        
        # Split data before TF-IDF to prevent data leakage
        X_train_text, X_test_text, y_train, y_test = train_test_split(
            processed_texts, data['sentiment'], test_size=0.1, random_state=42, 
            stratify=data['sentiment']
        )
        
        status_text.text("Building SMOTE Pipeline...")
        
        # Create SMOTE pipeline (following notebook implementation)
        pipeline = ImbPipeline([
            ('tfidf', TfidfVectorizer(
                max_features=1000,
                min_df=2,
                max_df=0.85,
                ngram_range=(1, 2),
                lowercase=False,
                strip_accents='unicode',
                norm='l2',
                sublinear_tf=True,
            )),
            ('smote', SMOTE(random_state=42)),  # Key improvement from notebook
            ('svm', SVC(
                C=0.1,                   # Optimal from GridSearchCV
                kernel='linear',         # Confirmed optimal
                gamma='scale',           # Confirmed optimal
                probability=True,
                random_state=42
            ))
        ])
        
        status_text.text("Training SMOTE Pipeline...")
        # Fit pipeline on training text (not TF-IDF vectors)
        pipeline.fit(X_train_text, y_train)
        
        status_text.text("Evaluating model...")
        y_pred = pipeline.predict(X_test_text)
        
        # Clear progress indicators
        status_text.empty()
        progress_bar.empty()
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, pos_label="POSITIF")
        recall = recall_score(y_test, y_pred, pos_label="POSITIF")
        f1 = f1_score(y_test, y_pred, pos_label="POSITIF")
        cm = confusion_matrix(y_test, y_pred)
        
        # Extract components for compatibility
        tfidf = pipeline.named_steps['tfidf']
        svm = pipeline.named_steps['svm']
        
        # Create test vectors for compatibility
        X_test = tfidf.transform(X_test_text)
        
        return pipeline, accuracy, precision, recall, f1, cm, X_test, y_test, tfidf, svm
        
    except Exception as e:
        st.error(f"Error in model training: {str(e)}")
        return None, 0, 0, 0, 0, None, None, None, None, None


def train_model_silent(data: pd.DataFrame, preprocessing_options: Optional[Dict] = None, 
                      batch_size: int = 1000) -> Tuple:
    """
    Train model without progress feedback (for background processing).
    
    Args:
        data: Training data
        preprocessing_options: Preprocessing configuration
        batch_size: Batch size for processing
        
    Returns:
        Tuple of (pipeline, accuracy, precision, recall, f1, confusion_matrix, X_test, y_test, tfidf, svm)
    """
    if preprocessing_options is None:
        preprocessing_options = DEFAULT_PREPROCESSING_OPTIONS.copy()
    
    try:
        processed_texts = []
        
        # Batch preprocessing without progress feedback
        for i in range(0, len(data), batch_size):
            batch_end = min(i + batch_size, len(data))
            batch = data.iloc[i:batch_end]
            batch_processed = []
            
            for text in batch['review_text']:
                try:
                    processed = preprocess_text(text, preprocessing_options)
                    batch_processed.append(processed)
                except Exception:
                    batch_processed.append(str(text))
            
            processed_texts.extend(batch_processed)
        
        # Split data before TF-IDF to prevent data leakage
        X_train_text, X_test_text, y_train, y_test = train_test_split(
            processed_texts, data['sentiment'], test_size=0.2, random_state=42, 
            stratify=data['sentiment']
        )
        
        # Create SMOTE pipeline
        pipeline = ImbPipeline([
            ('tfidf', TfidfVectorizer(
                max_features=1000,
                min_df=2,
                max_df=0.85,
                ngram_range=(1, 2),
                lowercase=False,
                strip_accents='unicode',
                norm='l2',
                sublinear_tf=True,
            )),
            ('smote', SMOTE(random_state=42)),
            ('svm', SVC(
                C=0.1,
                kernel='linear',
                gamma='scale',
                probability=True,
                random_state=42
            ))
        ])
        
        # Fit pipeline on training text
        pipeline.fit(X_train_text, y_train)
        y_pred = pipeline.predict(X_test_text)
        
        # Calculate metrics
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, pos_label="POSITIF")
        recall = recall_score(y_test, y_pred, pos_label="POSITIF")
        f1 = f1_score(y_test, y_pred, pos_label="POSITIF")
        cm = confusion_matrix(y_test, y_pred)
        
        # Extract components for compatibility
        tfidf_component = pipeline.named_steps['tfidf']
        svm_component = pipeline.named_steps['svm']
        
        # Create test vectors for compatibility
        X_test = tfidf_component.transform(X_test_text)
        
        return pipeline, accuracy, precision, recall, f1, cm, X_test, y_test, tfidf_component, svm_component
        
    except Exception as e:
        st.error(f"Error in model training: {str(e)}")
        raise e

# ==============================================================================
# MODEL SAVING AND LOADING FUNCTIONS
# ==============================================================================

def save_model_and_vectorizer(pipeline: Any, tfidf: Any, model_dir: str = "models") -> Tuple[str, str]:
    """
    Save pipeline and TF-IDF vectorizer with metadata.
    
    Args:
        pipeline: Trained pipeline
        tfidf: TF-IDF vectorizer
        model_dir: Directory to save models
        
    Returns:
        Tuple of (model_path, vectorizer_path)
    """
    os.makedirs(model_dir, exist_ok=True)
    model_path = os.path.join(model_dir, "svm.pkl")
    vectorizer_path = os.path.join(model_dir, "tfidf.pkl")
    metadata_path = os.path.join(model_dir, "model_metadata.txt")
    
    # Save models - handle both Pipeline types
    if hasattr(pipeline, 'named_steps'):
        # For ImbPipeline or regular Pipeline
        classifier = pipeline.named_steps.get('svm', pipeline.named_steps.get('classifier'))
        joblib.dump(classifier, model_path)
    else:
        # Fallback for direct classifier
        joblib.dump(pipeline, model_path)
    
    joblib.dump(tfidf, vectorizer_path)
    
    # Save metadata
    with open(metadata_path, 'w') as f:
        f.write(f"sklearn_version: {sklearn.__version__}\n")
        f.write(f"model_saved_at: {datetime.now().isoformat()}\n")
        f.write(f"model_type: SVM with TF-IDF and SMOTE\n")
        f.write(f"pipeline_type: ImbPipeline\n")
        f.write(f"imbalanced_handling: SMOTE\n")
    
    return model_path, vectorizer_path


def save_model_and_vectorizer_predict(pipeline: Any, tfidf_vectorizer: Any, 
                                     model_dir: str = "models") -> Tuple[str, str]:
    """
    Save model specifically for sentiment prediction.
    
    Args:
        pipeline: Complete pipeline for prediction
        tfidf_vectorizer: TF-IDF vectorizer
        model_dir: Directory to save models
        
    Returns:
        Tuple of (model_path, vectorizer_path)
    """
    os.makedirs(model_dir, exist_ok=True)
    
    # Save the complete pipeline for prediction
    model_path = os.path.join(model_dir, "svm_model_predict.pkl")
    vectorizer_path = os.path.join(model_dir, "tfidf_vectorizer_predict.pkl")
    
    # For prediction model, save the complete pipeline
    joblib.dump(pipeline, model_path)
    # Save the TF-IDF vectorizer separately for compatibility
    joblib.dump(tfidf_vectorizer, vectorizer_path)
    
    # Save metadata specific to prediction model
    metadata = {
        'model_type': 'Complete_Pipeline_with_SMOTE',
        'vectorizer_type': 'TF-IDF',
        'training_date': datetime.now().isoformat(),
        'sklearn_version': joblib.__version__,
        'version': 'prediction_complete_pipeline',
        'purpose': 'Sentiment prediction with complete pipeline',
        'target_module': 'Prediksi_Sentimen.py',
        'preprocessing': 'Handled by complete pipeline including SMOTE',
        'imbalanced_handling': 'SMOTE (Synthetic Minority Oversampling)'
    }
    
    metadata_path = os.path.join(model_dir, "model_metadata_predict.txt")
    with open(metadata_path, 'w') as f:
        for key, value in metadata.items():
            f.write(f"{key}: {value}\n")
    
    return model_path, vectorizer_path


def load_saved_model(model_dir: str = "models") -> Tuple[Any, Any]:
    """
    Load saved SVM model and TF-IDF vectorizer with version compatibility handling.
    
    Args:
        model_dir: Directory containing saved models
        
    Returns:
        Tuple of (svm_model, tfidf_vectorizer) or (None, None) if loading fails
    """
    model_path = os.path.join(model_dir, "svm.pkl")
    vectorizer_path = os.path.join(model_dir, "tfidf.pkl")
    
    if os.path.exists(model_path) and os.path.exists(vectorizer_path):
        try:
            # Suppress sklearn version warnings during model loading
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=UserWarning, module="sklearn")
                warnings.filterwarnings("ignore", message=".*Trying to unpickle estimator.*")
                
                svm_model = joblib.load(model_path)
                tfidf_vectorizer = joblib.load(vectorizer_path)
                
                # Validate that models are functional
                if hasattr(svm_model, 'predict') and hasattr(tfidf_vectorizer, 'transform'):
                    return svm_model, tfidf_vectorizer
                else:
                    return svm_model, tfidf_vectorizer
                    
        except Exception:
            return None, None
    return None, None


def load_prediction_model(model_dir: str = "models") -> Tuple[Any, Any]:
    """
    Load model specifically for sentiment prediction with enhanced compatibility.
    
    Args:
        model_dir: Directory containing saved models
        
    Returns:
        Tuple of (svm_model, tfidf_vectorizer) or (None, None) if loading fails
    """
    model_path = os.path.join(model_dir, "svm_model_predict.pkl")
    vectorizer_path = os.path.join(model_dir, "tfidf_vectorizer_predict.pkl")
    
    if os.path.exists(model_path) and os.path.exists(vectorizer_path):
        try:
            # Suppress all sklearn warnings during model loading
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")
                
                # Load with error handling for version compatibility
                try:
                    svm_model = joblib.load(model_path)
                except Exception as load_error:
                    st.error(f"Failed to load SVM model: {str(load_error)}")
                    # Try alternative loading method
                    try:
                        import pickle
                        with open(model_path, 'rb') as f:
                            svm_model = pickle.load(f)
                    except Exception as pickle_error:
                        st.error(f"Alternative loading also failed: {str(pickle_error)}")
                        return None, None
                
                try:
                    tfidf_vectorizer = joblib.load(vectorizer_path)
                except Exception as load_error:
                    st.error(f"Failed to load TF-IDF vectorizer: {str(load_error)}")
                    # Try alternative loading method
                    try:
                        import pickle
                        with open(vectorizer_path, 'rb') as f:
                            tfidf_vectorizer = pickle.load(f)
                    except Exception as pickle_error:
                        st.error(f"Alternative vectorizer loading failed: {str(pickle_error)}")
                        return None, None
                
                # Validate models functionality
                if (hasattr(svm_model, 'predict') and 
                    hasattr(tfidf_vectorizer, 'transform')):
                    
                    # Test basic functionality with proper preprocessing
                    try:
                        # Test with preprocessed text, not raw text
                        test_text_raw = "aplikasi ini bagus sekali"
                        test_text_processed = preprocess_text(test_text_raw, DEFAULT_PREPROCESSING_OPTIONS)
                        
                        # Ensure we have valid processed text
                        if not test_text_processed or not test_text_processed.strip():
                            test_text_processed = "aplikasi bagus"
                        
                        # Transform text to vector
                        test_vector = tfidf_vectorizer.transform([test_text_processed])
                        
                        # Make prediction - handle both pipeline and direct model
                        if hasattr(svm_model, 'predict'):
                            test_prediction = svm_model.predict(test_vector)
                        else:
                            # Fallback for pipeline objects
                            test_prediction = svm_model.predict([test_text_processed])
                        
                        st.success("‚úÖ Models loaded and validated successfully")
                        return svm_model, tfidf_vectorizer
                        
                    except Exception as test_error:
                        # Don't fail validation if models load correctly but test fails
                        st.info(f"‚ÑπÔ∏è Models loaded successfully (test skipped: {str(test_error)})")
                        return svm_model, tfidf_vectorizer
                else:
                    st.warning("‚ö†Ô∏è Models missing required methods")
                    return svm_model, tfidf_vectorizer  # Return anyway, might still work
                    
        except Exception as e:
            st.error(f"‚ö†Ô∏è Critical error loading prediction model: {str(e)}")
            return None, None
    else:
        st.warning("‚ö†Ô∏è Prediction model not found. Please train the model first.")
        return None, None


def check_sklearn_version_compatibility() -> bool:
    """
    Check if current sklearn version is compatible with saved models.
    
    Returns:
        True if compatible, False otherwise
    """
    try:
        import sklearn
        version = sklearn.__version__
        major, minor, patch = map(int, version.split('.'))
        
        # Compatible versions: 1.3.x to 1.5.x
        if major == 1 and 3 <= minor <= 5:
            return True
        else:
            st.warning(f"‚ö†Ô∏è sklearn version {version} may have compatibility issues")
            return False
    except Exception as e:
        st.error(f"Error checking sklearn version: {str(e)}")
        return False


def safe_model_load(file_path: str, model_type: str = "model") -> Any:
    """
    Safely load a pickled model with multiple fallback strategies.
    
    Args:
        file_path: Path to the pickled model file
        model_type: Type of model for error reporting
        
    Returns:
        Loaded model or None if loading fails
    """
    if not os.path.exists(file_path):
        st.error(f"‚ùå {model_type} file not found: {file_path}")
        return None
        
    loading_strategies = [
        ("joblib", lambda: joblib.load(file_path)),
        ("pickle", lambda: pickle.load(open(file_path, 'rb'))),
    ]
    
    for strategy_name, load_func in loading_strategies:
        try:
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore")
                model = load_func()
                st.success(f"‚úÖ {model_type} loaded successfully using {strategy_name}")
                return model
        except Exception as e:
            st.warning(f"‚ö†Ô∏è {strategy_name} loading failed for {model_type}: {str(e)}")
            continue
    
    st.error(f"‚ùå All loading strategies failed for {model_type}")
    return None


def check_model_compatibility() -> Tuple[bool, str]:
    """
    Check if saved models are compatible with current sklearn version.
    Enhanced version with proper error handling and validation.
    
    Returns:
        Tuple of (is_compatible, error_message)
    """
    try:
        # Check sklearn version first
        if not check_sklearn_version_compatibility():
            return True, "Version check passed with warnings"  # Don't fail on version warnings
            
        # Try to load models quickly to test compatibility
        model_path = os.path.join("models", "svm_model_predict.pkl")
        vectorizer_path = os.path.join("models", "tfidf_vectorizer_predict.pkl")
        
        if not (os.path.exists(model_path) and os.path.exists(vectorizer_path)):
            return False, "Model files not found"
            
        # Suppress all warnings during compatibility check
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore")
            
            try:
                # Load models without strict version checking
                test_model = joblib.load(model_path)
                test_vectorizer = joblib.load(vectorizer_path)
                
                # Skip functional test that causes CSR matrix error
                # Just check if objects have required methods
                if (hasattr(test_model, 'predict') and 
                    hasattr(test_vectorizer, 'transform')):
                    return True, "Models loaded successfully"
                else:
                    return True, "Models loaded but may have limited functionality"
                    
            except Exception as load_error:
                # Try with pickle as fallback
                try:
                    with open(model_path, 'rb') as f:
                        test_model = pickle.load(f)
                    with open(vectorizer_path, 'rb') as f:
                        test_vectorizer = pickle.load(f)
                    
                    return True, "Models loaded using fallback method"
                    
                except Exception as pickle_error:
                    return False, f"Failed to load models: {str(load_error)}"
            
    except Exception as e:
        # Don't fail compatibility check on minor issues
        return True, f"Compatibility check completed with warnings: {str(e)}"


# ==============================================================================
# MODEL MANAGEMENT FUNCTIONS
# ==============================================================================

def get_or_train_model(data: pd.DataFrame, preprocessing_options: Optional[Dict] = None, 
                      batch_size: int = 1000) -> Tuple:
    """
    Load pre-trained models or raise error if not available.
    This function expects models to be ready and will not perform training.
    
    Args:
        data: Training data for evaluation
        preprocessing_options: Preprocessing configuration
        batch_size: Batch size for processing
        
    Returns:
        Tuple of model components and metrics
    """
    # Try to load existing model
    svm_model, tfidf_vectorizer = load_saved_model()
        
    if svm_model is not None and tfidf_vectorizer is not None:
        # Model ready - prepare data for evaluation
        tfidf = tfidf_vectorizer
        svm = svm_model
        
        # Prepare text data for evaluation
        try:
            # Use preprocessed column if available
            if 'teks_preprocessing' in data.columns and not data['teks_preprocessing'].isna().all():
                processed_texts = data['teks_preprocessing'].astype(str).tolist()
                processed_texts = [str(text) if text is not None else "" for text in processed_texts]
            else:
                # Perform preprocessing on-the-fly if column doesn't exist
                if preprocessing_options is None:
                    preprocessing_options = DEFAULT_PREPROCESSING_OPTIONS.copy()
                
                processed_texts = []
                for text in data['review_text'].astype(str):
                    try:
                        processed_text = preprocess_text(text, preprocessing_options)
                        processed_texts.append(str(processed_text) if processed_text is not None else "")
                    except Exception:
                        # Fallback to raw text if preprocessing fails
                        processed_texts.append(str(text) if text is not None else "")
            
            # Validate processed texts
            processed_texts = [text for text in processed_texts if text and text.strip()]
            if not processed_texts:
                raise ValueError("No valid texts after preprocessing")
            
            X = tfidf.transform(processed_texts)
            y = data['sentiment'].iloc[:len(processed_texts)]  # Adjust y to match processed_texts length
            
        except Exception as e:
            # Fallback to raw text
            try:
                processed_texts = data['review_text'].astype(str).fillna('').tolist()
                processed_texts = [str(text) for text in processed_texts if str(text).strip()]
                X = tfidf.transform(processed_texts)
                y = data['sentiment'].iloc[:len(processed_texts)]
            except Exception:
                raise e  # Re-raise original error
        
        # Evaluate model performance
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        y_pred = svm.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, pos_label="POSITIF")
        recall = recall_score(y_test, y_pred, pos_label="POSITIF")
        f1 = f1_score(y_test, y_pred, pos_label="POSITIF")
        cm = confusion_matrix(y_test, y_pred)
        
        pipeline = Pipeline([
            ('vectorizer', tfidf),
            ('classifier', svm)
        ])
        
        return pipeline, accuracy, precision, recall, f1, cm, X_test, y_test, tfidf, svm
    else:
        # Model not found
        st.error("‚ùå Main model not found! Please restart the application for model retraining.")
        st.stop()


def quick_model_check() -> Tuple[bool, str, int]:
    """
    Quick check if all models are ready and compatible without UI elements.
    
    Returns:
        Tuple of (models_ready, compatibility_status, data_count)
    """
    try:
        # Check main models
        svm_model, tfidf_vectorizer = load_saved_model()
        main_models_exist = (svm_model is not None and tfidf_vectorizer is not None)
        
        # Check prediction models  
        prediction_model, prediction_vectorizer = load_prediction_model()
        prediction_models_exist = (prediction_model is not None and prediction_vectorizer is not None)
        
        # Check compatibility
        is_compatible, compatibility_msg = check_model_compatibility()
        
        # Load data count
        data = load_sample_data()
        data_count = len(data) if not data.empty else 0
        
        models_ready = main_models_exist and prediction_models_exist and is_compatible
        
        return models_ready, compatibility_msg, data_count
        
    except Exception as e:
        return False, f"Error checking models: {str(e)}", 0


def check_and_prepare_models_with_progress() -> Tuple[bool, bool, Dict]:
    """
    Unified function for checking and preparing models with progress feedback.
    
    Returns:
        Tuple of (models_ready, should_show_ui, preparation_data)
    """
    # Quick check first - no UI elements
    models_ready, compatibility_msg, data_count = quick_model_check()
    
    if models_ready:
        # Models are ready, no UI needed
        return True, False, {
            'data_count': data_count,
            'status': 'ready',
            'message': f'All models ready with {data_count:,} training samples'
        }
    
    # Models need preparation - show UI
    svm_model, tfidf_vectorizer = load_saved_model()
    prediction_model, prediction_vectorizer = load_prediction_model()
    
    main_models_exist = (svm_model is not None and tfidf_vectorizer is not None)
    prediction_model_exists = (prediction_model is not None and prediction_vectorizer is not None)
    
    if not main_models_exist or not prediction_model_exists:
        # Show model preparation notification
        if not main_models_exist and not prediction_model_exists:
            st.warning("‚ö†Ô∏è **Models not found, starting complete model training...**")
        elif not main_models_exist:
            st.warning("‚ö†Ô∏è **Main model not found, starting training...**")
        else:
            st.warning("‚ö†Ô∏è **Prediction model not found, starting preparation...**")
        
        # Create progress container
        progress_container = st.container()
        with progress_container:
            st.markdown("üîÑ **Model Preparation Process:**")
            
            # Load data for training
            data = load_sample_data()
            if data.empty:
                st.error("‚ùå Training data not found!")
                return False, True, {'status': 'error', 'message': 'No training data found'}
                
            preprocessing_options = DEFAULT_PREPROCESSING_OPTIONS.copy()
            
            # Train models with progress feedback
            with st.spinner("ü§ñ Preparing sentiment analysis models..."):
                try:
                    if not main_models_exist:
                        # Train main models
                        st.write("üìà **Stage 1:** Training main model...")
                        pipeline, accuracy, precision, recall, f1, cm, X_test, y_test, tfidf, svm = train_model_silent(data, preprocessing_options)
                        save_model_and_vectorizer(pipeline, tfidf)
                        
                        st.write("üéØ **Stage 2:** Preparing prediction model...")
                        save_model_and_vectorizer_predict(pipeline, tfidf)
                    else:
                        # Load existing models for metrics
                        pipeline, accuracy, precision, recall, f1, cm, X_test, y_test, tfidf, svm = get_or_train_model(data, preprocessing_options)
                    
                    if not prediction_model_exists:
                        # Create prediction-specific model
                        st.write("üéØ **Stage 3:** Preparing prediction model...")
                        save_model_and_vectorizer_predict(pipeline, tfidf)
                    
                    # Clear progress container
                    progress_container.empty()
                    
                    # Success notification
                    st.toast(f"‚úÖ All models prepared successfully! Accuracy: {accuracy:.2%}", icon="‚úÖ")
                    st.success(f"""
                    üéâ **Sentiment analysis model system successfully prepared!**
                    
                    üìä **Model Performance:**
                    - **Accuracy:** {accuracy:.2%}
                    - **Precision:** {precision:.2%}
                    - **Recall:** {recall:.2%}
                    - **F1-Score:** {f1:.2%}
                    
                    ‚úÖ **Available Models:**
                    - üîß Main model for general analysis
                    - ‚öôÔ∏è Model without SMOTE for optimal performance
                    - üéØ Specialized model for real-time prediction
                    
                    üöÄ **All models ready for use!**
                    """)
                    
                    return True, True, {
                        'status': 'success',
                        'accuracy': accuracy,
                        'precision': precision,
                        'recall': recall,
                        'f1': f1,
                        'data_count': len(data)
                    }
                    
                except Exception as e:
                    progress_container.empty()
                    st.error(f"‚ùå Failed to prepare models: {str(e)}")
                    st.toast("‚ùå Model preparation failed!", icon="‚ùå")
                    return False, True, {'status': 'error', 'message': str(e)}
    else:
        # Models exist, check compatibility
        is_compatible, compatibility_msg = check_model_compatibility()
        
        if not is_compatible:
            st.warning(f"‚ö†Ô∏è **Model compatibility issue:** {compatibility_msg}")
            
            progress_container = st.container()
            with progress_container:
                st.markdown("üîÑ **Updating models to compatible version...**")
                
                data = load_sample_data()
                preprocessing_options = DEFAULT_PREPROCESSING_OPTIONS.copy()
                
                with st.spinner("üîÑ Updating models..."):
                    try:
                        st.write("üìà **Stage 1:** Updating main model...")
                        pipeline, accuracy, precision, recall, f1, cm, X_test, y_test, tfidf, svm = train_model_silent(data, preprocessing_options)
                        save_model_and_vectorizer(pipeline, tfidf)
                        
                        st.write("üéØ **Stage 2:** Updating prediction model...")
                        save_model_and_vectorizer_predict(pipeline, tfidf)
                        
                        progress_container.empty()
                        st.toast("‚úÖ All models updated successfully!", icon="‚úÖ")
                        st.success("üéâ **All models successfully updated and ready for use!**")
                        return True, True, {
                            'status': 'updated',
                            'accuracy': accuracy,
                            'data_count': len(data)
                        }
                        
                    except Exception as e:
                        progress_container.empty()
                        st.error(f"‚ùå Failed to update models: {str(e)}")
                        st.toast("‚ùå Model update failed!", icon="‚ùå") 
                        return False, True, {'status': 'error', 'message': str(e)}
        else:
            # All models are ready and compatible
            return True, False, {
                'status': 'ready',
                'data_count': data_count,
                'message': 'All models ready and compatible'
            }

# ==============================================================================
# PREDICTION FUNCTIONS
# ==============================================================================

def predict_sentiment(text: str, pipeline: Any, preprocessing_options: Optional[Dict] = None, 
                     use_prediction_model: bool = False, svm_model: Any = None, 
                     tfidf_vectorizer: Any = None) -> Dict[str, Any]:
    """
    Predict sentiment with improved SVM bug fixes (argmax vs decision function).
    
    Args:
        text: Text to predict
        pipeline: Pipeline model to use (for regular model)
        preprocessing_options: Preprocessing options (optional)
        use_prediction_model: If True, use specialized prediction model
        svm_model: Specialized SVM model for prediction (if use_prediction_model=True)
        tfidf_vectorizer: TF-IDF vectorizer for prediction (if use_prediction_model=True)
    
    Returns:
        Dictionary with sentiment prediction results
    """
    if preprocessing_options is None:
        preprocessing_options = DEFAULT_PREPROCESSING_OPTIONS.copy()
    
    try:
        # If using specialized prediction model
        if use_prediction_model and svm_model is not None and tfidf_vectorizer is not None:
            # For specialized prediction model, perform preprocessing
            processed_text = preprocess_text(text, preprocessing_options)
            if not processed_text.strip():
                return {
                    'sentiment': 'NETRAL',
                    'confidence': 0,
                    'probabilities': {'POSITIF': 0, 'NEGATIF': 0}
                }
            
            # The svm_model is actually the complete pipeline, so we can use it directly
            probabilities = svm_model.predict_proba([processed_text])[0]
            predicted_class_idx = probabilities.argmax()
            prediction = svm_model.classes_[predicted_class_idx]
            confidence = probabilities[predicted_class_idx]
            
            # Create probability mapping based on classes
            prob_dict = {}
            for i, class_name in enumerate(svm_model.classes_):
                prob_dict[class_name] = float(probabilities[i])
                
        else:
            # Use full preprocessing for regular model
            processed_text = preprocess_text(text, preprocessing_options)
            if not processed_text.strip():
                return {
                    'sentiment': 'NETRAL',
                    'confidence': 0,
                    'probabilities': {'POSITIF': 0, 'NEGATIF': 0}
                }
            
            # Get probabilities from model pipeline
            probabilities = pipeline.predict_proba([processed_text])[0]
            
            # ‚úÖ SVM BUG FIX: Use argmax from probabilities, not decision function
            # This ensures prediction is consistent with highest probability
            predicted_class_idx = probabilities.argmax()
            prediction = pipeline.classes_[predicted_class_idx]
            confidence = probabilities[predicted_class_idx]
            
            # Create probability mapping based on classes
            prob_dict = {}
            for i, class_name in enumerate(pipeline.classes_):
                prob_dict[class_name] = float(probabilities[i])
        
        return {
            'sentiment': prediction,
            'confidence': float(confidence),
            'probabilities': prob_dict
        }
        
    except Exception as e:
        # Return informative error without displaying to UI
        error_msg = f"Prediction error: {str(e)}"
        
        # Log error for debugging
        print(f"ERROR in predict_sentiment: {error_msg}")
        print(f"Traceback: {traceback.format_exc()}")
        
        return {
            'sentiment': 'ERROR',
            'confidence': 0,
            'probabilities': {'POSITIF': 0, 'NEGATIF': 0},
            'error': error_msg
        }

# ==============================================================================
# UI AND UTILITY FUNCTIONS
# ==============================================================================

def get_table_download_link(df: pd.DataFrame, filename: str, text: str) -> str:
    """
    Generate download link for DataFrame as CSV.
    
    Args:
        df: DataFrame to download
        filename: Filename for download
        text: Link text
        
    Returns:
        HTML download link
    """
    try:
        csv = df.to_csv(index=False)
        b64 = base64.b64encode(csv.encode()).decode()
        return f'<a href="data:file/csv;base64,{b64}" download="{filename}.csv">{text}</a>'
    except Exception as e:
        st.error(f"Error generating download link: {str(e)}")
        return ''


def display_model_metrics(accuracy: float, precision: float, recall: float, 
                         f1: float, confusion_mat: np.ndarray) -> None:
    """
    Display model performance metrics in sidebar.
    
    Args:
        accuracy: Model accuracy
        precision: Model precision
        recall: Model recall
        f1: Model F1-score
        confusion_mat: Confusion matrix
    """
    with st.sidebar.expander("üèÜ Model Metrics", expanded=False):
        st.write(f"‚úÖ Accuracy: {accuracy:.4f}")
        st.write(f"‚úÖ Precision: {precision:.4f}")
        st.write(f"‚úÖ Recall: {recall:.4f}")
        st.write(f"‚úÖ F1-Score: {f1:.4f}")
        
        fig, ax = plt.subplots(figsize=(4, 3))
        ax.set_title("Confusion Matrix")
        im = ax.imshow(confusion_mat, cmap='Blues')
        plt.colorbar(im, ax=ax)
        ax.set_xticks([0, 1])
        ax.set_yticks([0, 1])
        ax.set_xticklabels(["NEGATIF", "POSITIF"])
        ax.set_yticklabels(["NEGATIF", "POSITIF"])
        ax.set_xlabel("Predicted")
        ax.set_ylabel("True")
        
        for i in range(2):
            for j in range(2):
                ax.text(j, i, confusion_mat[i, j], ha="center", va="center", 
                       color="white" if confusion_mat[i, j] > confusion_mat.max()/2 else "black")
        st.pyplot(fig)


def render_model_preparation_page() -> None:
    """
    Render model preparation page with improved UI.
    This function replaces model_preparation_page() in main.py for consistency.
    """
    st.markdown(
        """
        <style>
        [data-testid='stSidebar'], [data-testid='collapsedControl'] {
            display: none !important;
        }
        </style>
        """,
        unsafe_allow_html=True
    )
    
    # Page header
    st.markdown("# ü§ñ Sentiment Analysis Model Preparation")
    st.markdown("---")
    
    # Welcome message
    user_email = st.session_state.get('user_email', 'User')
    st.markdown(f"### Welcome, **{user_email}**! üëã")
    
    st.info("""
    üîß **System is preparing AI models for GoRide sentiment analysis**
    
    üìù This process includes:
    - ‚úÖ Checking available models
    - ü§ñ Training models if needed  
    - üìä Validating model performance
    - üöÄ Preparing analysis tools
    
    *This process is done only once or when models need updating.*
    """)
    
    # Status container
    status_container = st.container()
    
    # Check if models are being prepared
    if not st.session_state.get('model_preparation_started', False):
        with status_container:
            if st.button("üöÄ **Start Model Preparation**", type="primary", use_container_width=True):
                st.session_state['model_preparation_started'] = True
                st.rerun()
    else:
        # Model preparation in progress
        with status_container:
            st.markdown("### üîÑ Preparing Models...")
            
            try:
                # Show preparation progress
                models_ready, should_show_ui, preparation_data = check_and_prepare_models_with_progress()
                
                if models_ready:
                    st.session_state['models_prepared'] = True
                    st.session_state['model_preparation_completed'] = True
                    
                    # Success message with celebration
                    st.balloons()
                    st.success("üéâ **Models prepared successfully!**")
                    
                    st.markdown("---")
                    st.markdown("### ‚úÖ Preparation Complete!")
                    
                    col1, col2, col3 = st.columns([1, 2, 1])
                    with col2:
                        if st.button("üéØ **Continue to Dashboard**", type="primary", use_container_width=True):
                            st.session_state['ready_for_tools'] = True
                            st.rerun()
                            
                    st.markdown("*You are ready to use all sentiment analysis features!*")
                else:
                    st.error("‚ùå **Failed to prepare models**")
                    col1, col2 = st.columns([1, 1])
                    with col1:
                        if st.button("üîÑ Try Again"):
                            st.session_state['model_preparation_started'] = False
                            st.rerun()
                    with col2:
                        if st.button("üö™ Logout"):
                            from ui.auth import auth
                            auth.logout()
                            
            except Exception as e:
                st.error(f"‚ùå **Error preparing models:** {str(e)}")
                col1, col2 = st.columns([1, 1])
                with col1:
                    if st.button("üîÑ Try Again"):
                        st.session_state['model_preparation_started'] = False
                        st.rerun()
                with col2:
                    if st.button("üö™ Logout"):
                        from ui.auth import auth
                        auth.logout()

# ==============================================================================
# END OF FILE
# ==============================================================================
